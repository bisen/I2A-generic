import tensorflow as tf
import numpy as np
import gym

## STABLE LOG
def log(x):
    return tf.log(tf.maximum(x, 1e-5))

## HYPERPARAMETERS
GAMMA = 0.99
LEARNING_RATE = 0.0002
ENTROPY_FACTOR = 0.05
EPOCHS = 10000
TIMESTEPS = 1000
DECAY = 0.99
EPSILON = 1e-5
REWARD_FACTOR = 0.1
SAVE_EVERY = 100

class A2C:
    def __init__(self, game):
        self.game = gym.make(game)
        self.num_actions = 2
        self.state_size = self.game.observation_space.shape

        self.state_input = tf.placeholder(tf.float32, [None] + list(self.state_size))

        # Define any additional placeholders needed for training your agent here:

        self.rewards = tf.placeholder( shape = [ None ], dtype = tf.float32 )
        self.actions = tf.placeholder( shape = [ None ], dtype = tf.int32 )

        self.common = self.common()
        self.state_value = self.critic()
        self.actor_probs = self.actor()
        self.loss_val = self.loss()
        self.train_op = self.optimizer()
        self.session = tf.Session()
        self.session.run(tf.global_variables_initializer())

        # For saving/loading models
        self.saver = tf.train.Saver()

    # Load the last saved checkpoint during training or used by test
    def load_last_checkpoint(self):
        self.saver.restore(self.session, tf.train.latest_checkpoint('./'))

    def save_checkpoint(self):
        self.saver.save(self.session, './a2c_saved_model')

    def next_action(self, state):
        self.load_last_checkpoint()
        actDist = self.session.run( self.actor_probs, feed_dict={ self.state_input: np.array( [ state ] ) } )
        action_idx= np.random.choice( self.num_actions, 1, p=actDist[0] )[0]
        if action_idx == 0:
            action = 2
        elif action_idx == 1:
            action = 5
        return action

    def optimizer(self):
        """
        :return: Optimizer for your loss function
        """
        return tf.train.RMSPropOptimizer( LEARNING_RATE, decay=DECAY, epsilon=EPSILON ).minimize( self.loss_val )

    def common(self):
        resized_input = tf.image.convert_image_dtype(tf.image.resize_images(self.state_input[:,24:,:,:], [96,96]), tf.float32)
        h0 = tf.nn.elu(tf.layers.batch_normalization(tf.layers.conv2d(resized_input, 8, [7,7], (3,3), padding='valid', kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d())))
#        h1 = tf.nn.elu(tf.layers.batch_normalization(tf.layers.conv2d(h0, 1, [5,5], (2,2), padding='valid', kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d())))
#        h2 = tf.nn.elu(tf.layers.batch_normalization(tf.layers.conv2d(h1, 1, [5,5], (2,2), padding='valid', kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d())))
        h3 = tf.nn.elu(tf.layers.batch_normalization(tf.layers.dense(tf.layers.flatten(h0), 32, kernel_initializer=tf.contrib.layers.xavier_initializer())))
        return h3

    def critic(self):
        """
        Calculates the estimated value for every state in self.state_input. The critic should not depend on
        any other tensors besides self.state_input.
        :return: A tensor of shape [num_states] representing the estimated value of each state in the trajectory.
        """
        #import pdb; pdb.set_trace()
        output = tf.layers.dense(self.common, 1, kernel_initializer=tf.contrib.layers.xavier_initializer())
        return output

    def actor(self):
        """
        Calculates the action probabilities for every state in self.state_input. The actor should not depend on
        any other tensors besides self.state_input.
        :return: A tensor of shape [num_states, num_actions] representing the probability distribution
            over actions that is generated by your actor.
        """
        output = tf.layers.dense(self.common, self.num_actions, kernel_initializer=tf.contrib.layers.xavier_initializer())
        return tf.nn.softmax(output)


    def loss(self):
        """
        :return: A scalar tensor representing the combined actor and critic loss.
        """
        #import pdb; pdb.set_trace()
        indicies = tf.range(0, tf.shape(self.actor_probs)[0]) * self.num_actions + self.actions
        actProbs = tf.gather(tf.reshape(self.actor_probs, [-1]), indicies)
        # A(s, a) = Q(s, a) - V(s)
        advantage = self.rewards - self.state_value
        # loss for critic
        cLoss = tf.reduce_mean(tf.square(advantage))
        # loss for actor
        aLoss = -tf.reduce_mean(log(actProbs) * advantage)
        entropy = -tf.reduce_mean(actProbs * log(actProbs))
        return aLoss+cLoss + 0.05*entropy

    def train_episode(self):
        """
        train_episode will be called 1000 times by the autograder to train your agent. In this method,
        run your agent for a single episode, then use that data to train your agent. Feel free to
        add any return values to this method.
        """

        st = self.game.reset()
        states = []
        rewards = []
        actions = []

        for t in range(TIMESTEPS):
            #self.game.render()
            actDist = self.session.run( self.actor_probs, feed_dict={ self.state_input: np.array( [ st ] ) } )
            action_idx= np.random.choice( self.num_actions, 1, p=actDist[0] )[0]
            if action_idx == 0:
                action = 2
            elif action_idx == 1:
                action = 5

            st1, reward, done, _ = self.game.step(action)

            states.append(st)
            rewards.append(reward * REWARD_FACTOR)
            actions.append(action_idx)

            st = st1

            if done or t == TIMESTEPS - 1:
                d_r = [0]
                #import pdb; pdb.set_trace()
                reversed_rewards = rewards[:]
                reversed_rewards.reverse()
                for r in reversed_rewards:
                   d_r.append( d_r[ -1 ] * GAMMA + r )
                disRs = d_r[1:]
                disRs.reverse()

                _ , loss = self.session.run( [self.train_op, self.loss_val], feed_dict= { self.state_input: np.array( states ), self.rewards: np.array( disRs ), self.actions: np.array( actions ) } )
                print( "rewards: ", np.sum(rewards ))
                print( "loss: ", loss )
                break
        return


if __name__ == '__main__':
    # Change __main__ to train your agent for 1000 episodes and print the average reward over the last 100 episodes.
    # The code below is similar to what our autograder will be running.

    model = A2C('Pong-v0')
    for i in range(EPOCHS):
        model.train_episode()

        if i%SAVE_EVERY == 0:
            print("MODEL saved at iteration: ", i)
            model.save_checkpoint()
